\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\usepackage[nomain, acronym, symbols]{glossaries}
\usepackage[english]{babel}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{graphicx}

\loadglsentries{acronyms}

% TODO: Development
% TODO: Results and discussion
% TODO: Conclusion

\begin{document}

\title{Fine-tuning of LLaMA for Skin Lesion Classification and Medical Report Generation}

\author{
    \IEEEauthorblockN{1\textsuperscript{st} Eric Fernandes Evaristo}
    \IEEEauthorblockA{
        \textit{dept. of \acrlong{ine}} \\
        \textit{\acrlong{ufsc}} \\
        Florianópolis, Brazil \\
        eric.f.evaristo@posgrad.ufsc.br
    }
\and
    \IEEEauthorblockN{2\textsuperscript{nd} Aldo von Wangenheim}
    \IEEEauthorblockA{
        \textit{dept. of \acrlong{ine}} \\
        \textit{\acrlong{ufsc}} \\
        Florianópolis, Brazil \\
        aldo.vw@ufsc.br
    }
\and
    \IEEEauthorblockN{3\textsuperscript{rd} Rodrigo de Paula e Silva Ribeiro}
    \IEEEauthorblockA{
        \textit{dept. of \acrlong{ine}} \\
        \textit{\acrlong{ufsc}} \\
        Florianópolis, Brazil \\
        ribeiro.rodrigo@posgrad.ufsc.br
    }
}

\maketitle

\begin{abstract}
    The early detection of skin lesions is crucial for the treatment of severe diseases, such as skin cancer. However, these lesions are usually only diagnosed by dermatologists. Given that these professionals are not always available in regions with less healthcare infrastructure, it would be useful to have a tool capable of skin lesion classification and medical report generation. For this purpose, we fine-tuned \gls{llama} 3.2 11B with \gls{qlora} and \gls{lora}. The fine-tuning was performed in two scenarios, in the first scenario, we trained the model with the \gls{ham10000} dataset for classification tasks only, achieving an accuracy of up to 86.2\% with \gls{lora}. In the second scenario, the model was trained with the \gls{sttsc} dataset for both classification and report generation. The final models presented a classification accuracy of 45.2\% with \gls{qlora} and 44.5\% with \gls{lora}. This low performance can be explained by inconsistencies in the \gls{sttsc} dataset.
\end{abstract}

\begin{IEEEkeywords}
    % Some keywords won't need acronyms anyway
    Skin Lesions, Skin Cancer, MLLM, \gls{llama}, Fine-tuning, \gls{qlora}, \gls{lora}.
\end{IEEEkeywords}

\section{Introduction}

Skin lesions are symptoms of several diseases, most notably skin cancer. In Brazil, skin cancer is particularly relevant, accounting for 30\% of all malignant tumors \cite{skin_cancer_in_brazil}. Accurate and early classification of the lesion category is fundamental for adequate treatment. However, this is a complex task dependent on many visual aspects, thus requiring analysis by trained professionals, such as dermatologists \cite{habif2015clinical, skin_cancer_survival}.

The early detection of skin cancer also has a socio-economic factor. As noted in studies by \citet{skin_cancer_socioeconomic} and \citet{santos2020desigualdades}, patients who present with tumors in more advanced stages tend to come from a lower socio-economic background. This can be correlated with a lack of access to specialized healthcare and dermatologists for these patients. However, in the Brazilian context, primary healthcare is partially provided by \gls{acs} (Community Health Agents), who offer basic health assistance and are in direct contact with impoverished communities \cite{filgueiras2011agente}.

In this context, a user-friendly tool capable of classifying skin lesions and generating reports would be useful, as it could be used by \gls{acs} as a screening mechanism, thereby addressing the lack of access to dermatologists for part of the population. The requirements for such a tool could be addressed by a \gls{mllm}. Therefore, \gls{llama}-3.2-11B was chosen as the base model in this work for the experimental development of a skin lesion classifier and report generator.

The development was divided into two scenarios. In the first, the model was fine-tuned with the \gls{ham10000} dataset. In the second scenario, the model was fine-tuned with the \gls{sttsc} dataset. Both scenarios employed \gls{qlora} and \gls{lora}.

\subsection{Contributions}

The following contributions are presented in this work:

\begin{itemize}
    \item The development of a fine-tuned version of \gls{llama}-3.2-11B capable of classifying skin lesions and generating reports.
    \item A comparative analysis of the fine-tuned models with the base model.
    \item A comparative analysis of the efficiency of \gls{qlora} and \gls{lora} in fine-tuning.
\end{itemize}

\section{Basic Concepts}

This section defines the basic concepts applied in this research.

\subsection{Skin Lesion Images}

Skin lesion images can be classified into different categories. The procedures described by \citet{fotos_dermatologia} for obtaining these images present three categories: dermatoscopy, close-up and panoramic images. For the fine-tuning, only dermatoscopy and close-up images were used.

Dermatoscopy images are obtained with a dermatoscope, which is used to capture an image of the specific area of skin where the lesion is. Alternatively, close-up images can be obtained with a regular camera and a ruler, where they are used to take a picture of the region of the body where the skin lesion appears with a distance of 30cm from the skin lesion to the camera. \autoref{fig:skin_lesion_exams} presents examples of exams with the categories of images utilized in this work.

\begin{figure}[htbp]
\centerline{\includegraphics{figures/skin_lesion_exams.png}}
\caption{Example of a dermatoscopy (left) and a close-up (right) exam. Source: \citet{fotos_dermatologia}}
\label{fig:skin_lesion_exams}
\end{figure}

\subsection{Multimodal Large Language Models}

\glspl{mllm} are generative \gls{ai} models based on \glspl{llm} that are capable of processing information from multiple modalities, such as images, audio, and text. For the interpretation of multimodal data, \glspl{mllm} have modality encoders, which are integrated with the \gls{llm} through an input projector. Furthermore, \glspl{llm} and most modality encoders are based on transformers \cite{mllm_survey_2024}.

\subsection{Fine-tuning}

Fine-tuning can be defined as a training procedure with the objective of adapting a pre-trained model for a specific task or domain. With \gls{peft} techniques, such as \gls{qlora} and \gls{lora}, the process can be much less computationally intensive than full-parameter fine-tuning \cite{llm_survey_2023}.

The \gls{lora} method was introduced by \citet{hu2021lora}, utilizing the intrinsic rank of matrices. It introduces smaller trainable matrices while freezing the original model matrices during training, greatly reducing the amount of tunable parameters. After training, the smaller matrices can be added to the original ones, resulting in a fine-tuned model.

In order to further reduce the computational cost, \citet{qlora} introduced \gls{qlora}, a quantized version of \gls{lora}. With this method, low-precision numerical data types such as \gls{nf4} and \gls{bf16} are used.

\section{Related Works}

Over the past years, several papers have demonstrated applications of \glspl{mllm} to the analysis of medical images. In a systematic literature review, \citet{eric2025systematic} highlights the main advancements in this field. Overall, most \glspl{mllm} are generalist, operating over several medical domains. However, some models such as SkinGPT-4 and MpoxVLM focus on dermatological tasks \cite{zhou2023skingpt,cao2024mpoxvlm}.

What distinguishes this work from others is the utilization of \gls{llama}-3.2-11B, since other models focused on dermatology are still based on older \glspl{mllm}. Furthermore, a comparative analysis of \gls{qlora} and \gls{lora} in fine-tuning has not been presented in the existing literature.

\section{Methodology and Developement}

The following subsections will present the development scenarios. The resulting models from the first scenario were not utilized in the second one. This Methodology was implemented for a better assessment of the efficiency of different fine-tuning methods. The trainings were conducted with a NVIDIA H100 on a NVIDIA DGX H100 platform and with the framework Unsloth.

\subsection{First Scenario}

The first scenario is comprised of the fine-tuning of the base model with different dataset sizes and fine-tuning parameters. \gls{ham10000} was utilized as the dataset, it contains 13,354 dermatoscopy images with a resolution of \(600 \times 450\) pixels. This dataset is split between training, test and validation sections, with 9,577, 1,258 and 2,492 images, respectively. For the fine-tuning, the dataset was adapted to a conversational structure, where the user asks for the skin lesion classification in the image and the model answers accordingly.

\gls{llama}-3.2-11B was fine-tuned with 450, 900 and 5,000 samples from the training dataset. The hyperparameters were based on recommended values by Unsloth documentation. For each sample size, a \gls{qlora} and \gls{lora} were used.

\subsection{Second Scenario}

In the second scenario, the base model was fine-tuned with the \gls{sttsc} dataset. This dataset was created through the sanitization and structuring of a set of 31,243 images and 3,651 exams, resulting in a dataset with 4,736 samples for training and 1,167 samples for testing. Each sample consists of a pair of a image and a medical report with the skin lesion classification, a risk classification, a conclusion field with recommendations and a brief diagnostic. The data was also structured in a conversational pattern, where the user presents an image and asks for its information, requesting it to filled with a specific template and the model answers as required.

Here, the fine-tuning was initially performed only with \gls{qlora} in order to find ideal hyperparameters more efficiently. In total, eight models were trained with variations on the rank, \(\alpha_{\gls{lora}}\) and number of epochs. Besides, the hyperparameters were adjusted according to recommendations from \citet{tribes2023hyperparameter} and the Unsloth documentation. The resulting model with the highest accuracy in skin lesion classification was fine-tuned with a rank and \(\alpha_{\gls{lora}}\) of 128 and 3 epochs. These hyperparameters were then used for the fine-tuning with \gls{lora}.

\section{Results and Discussion}

\section{Conclusion}

\subsection{Future work}

\bibliographystyle{IEEEtranN}
\bibliography{references}

\end{document}

